{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Colormap of the notebook:*\n",
    "\n",
    "* <span style=\"color:red\">assignment problem</span>. The red color indicates the task that should be done\n",
    "* <span style=\"color:green\">debugging</span>. The green tells you what is expected outcome. Its primarily goal to help you get the correct answer\n",
    "* <span style=\"color:blue\">hints</span>.\n",
    "\n",
    "Assignment 2b ( DTD dataset )\n",
    "======================\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During this assignment we will consider DTD dataset and prepare it for further use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Describable Textures Dataset (DTD)**  \n",
    "*source*: https://www.robots.ox.ac.uk/~vgg/data/dtd/  \n",
    "*description*: DTD is a texture database, consisting of 5640 images, organized according to a list of 47 terms (categories) inspired from human perception. There are 120 images for each category. Image sizes range between 300x300 and 640x640.  \n",
    "*State-of-the-art accuracy*: $73.6\\%$ [https://arxiv.org/abs/1511.05197]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"> do in the terminal: </span>  \n",
    "\n",
    "<span style=\"color:red\"> 1. go the folder 'data' where assigments are </span>  \n",
    "\n",
    "<pre> > cd $ASSIGNMENT_DIR$/data </pre>  \n",
    "\n",
    "<span style=\"color:red\"> 2. get the data and perform train/test partition [YOU NEED ~ 700 MB FREE] </span>  \n",
    "<pre> > chmod +x get_dtd_dataset.sh </pre>  \n",
    "<pre> > ./get_dtd_dataset.sh </pre>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the associated webpage\n",
    "import IPython\n",
    "url = 'https://www.robots.ox.ac.uk/~vgg/data/dtd/'\n",
    "iframe = '<iframe src=' + url + ' width=\"100%\" height=500></iframe>'\n",
    "IPython.display.HTML(iframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compatability issues (python 2 & python 3)\n",
    "from __future__ import print_function\n",
    "from __future__ import division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# needed libs\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch stuff\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# to make interactive plotting possible\n",
    "%matplotlib inline\n",
    "# for auto-reloading external modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to the data\n",
    "path_data = 'data/dtd'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classes = os.listdir(os.path.join(path_data, 'train'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = len(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"There are \" + str(len(classes)) + \" classes:\")\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shape, size, ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Number of images for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for cls in classes[:5]:\n",
    "    for train_test in ['train', 'test']:\n",
    "        path_dir_cls = os.path.join(path_data, train_test, cls)\n",
    "        imgs_list = os.listdir(path_dir_cls)\n",
    "        print( \"[%s] %s : %d\" % (train_test, cls, len(imgs_list)))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All classes have the same amount of images 120, 80 images in train and 40 images in test.\n",
    "  \n",
    "So we have $ (80 + 40) * 47 = 5640$ images"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "width  [max, min]:  [271, 900]\n",
    "height [max, min]:  [231, 778]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The main message is that the image sizes are different and we have to deal with it when we will train NN on them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "indx_cls = np.random.randint(len(classes))\n",
    "cls = classes[indx_cls]\n",
    "\n",
    "class_images = os.listdir(os.path.join(path_data, 'train', cls))\n",
    "indx_im = np.random.randint(len(class_images))\n",
    "\n",
    "im_name = class_images[indx_im]\n",
    "im_full_name = os.path.join(path_data, 'train', cls, im_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "im = Image.open(im_full_name)\n",
    "plt.figure(figsize=(7, 7))\n",
    "plt.imshow(im)\n",
    "plt.title(\"[class]: \" + cls + \"\\n [im]: \" + im_name);\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look more closely "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"> You are invited to play with code below  </span>  \n",
    "\n",
    "<span style=\"color:red\"> - change 'samples_per_class' </span>  \n",
    "<span style=\"color:red\"> - look at different classes by changing 'classes_to_see' </span>  \n",
    "<span style=\"color:red\"> - try 'train' vs 'test' by changing 'train_test' variable </span>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "samples_per_class = 7\n",
    "classes_to_see = np.arange(20,28)\n",
    "#classes_to_see = [1,2,3]\n",
    "train_test = \"train\"\n",
    "#train_test = \"test\"\n",
    "for cls in [classes[i] for i in classes_to_see]:\n",
    "    class_images = os.listdir(os.path.join(path_data, train_test, cls))\n",
    "    class_images_pick = np.random.choice(class_images, samples_per_class, replace=False)\n",
    "    \n",
    "    plt.figure(figsize=(2 * samples_per_class, 5))    \n",
    "    for i, im_name in enumerate(class_images_pick):\n",
    "        plt_idx = i + 1\n",
    "        plt.subplot(1, samples_per_class, plt_idx)\n",
    "        im_full_name = os.path.join(path_data, train_test, cls, im_name)\n",
    "        im = Image.open(im_full_name)\n",
    "        plt.imshow(im)\n",
    "        plt.axis('off')\n",
    "        if i == 0:\n",
    "            im_x, im_y = im.size\n",
    "            plt.text(-400, im_y / 2, cls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is common in Computer Vision to make different image transforms.  \n",
    "*pytorch* (*torchvision* to be more precise) provides several commony used transforms and tools to combine these transforms.\n",
    "\n",
    "Plese visit for details\n",
    "https://github.com/pytorch/vision#transforms\n",
    "\n",
    "Let check different transformation\n",
    "\n",
    "First we define useful functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor2im(tensor_):\n",
    "    \"\"\" Bring the tensor back to image\"\"\"\n",
    "    return transforms.ToPILImage()(tensor_)\n",
    "\n",
    "def display_diff(im, im_transformed):\n",
    "    \"\"\" Display a difference between original image and transformed\"\"\"\n",
    "    plt.figure(figsize=(10,3))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(im)\n",
    "    plt.title('original')\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(im_transformed)\n",
    "    plt.title('transformed')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* load an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_dir = os.path.join(path_data, 'train', classes[8])\n",
    "im_full_name = im_dir + \"/\" + os.listdir(im_dir)[3]\n",
    "im = Image.open(im_full_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* define transform itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In words we define the following transformation:  \n",
    "1. crop the image at the center to have a region of the given size (224 in our case)\n",
    "2. randomly horizontally flips the given image with a probability of 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* apply transform and check the difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# apply transform\n",
    "im_transformed = transform(im)\n",
    "# display results\n",
    "display_diff(im, im_transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"> **[PROBLEM I]**: </span> \n",
    "\n",
    "<span style=\"color:red\">Define the following transform </span>  \n",
    "<span style=\"color:red\">1. crop the image at a random location to have a region of the given size (230)</span>  \n",
    "<span style=\"color:blue\">use 'RandomCrop'    https://github.com/pytorch/vision#randomcropsize-padding0</span>    \n",
    "<span style=\"color:red\">2. randomly horizontally flip the given image </span>  \n",
    "<span style=\"color:red\">3. Rescale the input image to the given 'size' (32) </span>  \n",
    "<span style=\"color:blue\"> consider to use 'Scale' https://github.com/pytorch/vision#scalesize-interpolationimagebilinear </span>  \n",
    "\n",
    "<span style=\"color:red\"> apply this transform and see the results </span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#YOUR CODE HERE\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.Scale(32)\n",
    "])\n",
    "# apply transform\n",
    "im_transformed = transform(im)\n",
    "# display results\n",
    "display_diff(im, im_transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another interesting tranformation is normalization, commonly used to normalize an image, prior to training  \n",
    "It operates on a Tensor, rather than an image and requires two params - mean & std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagenet_mean = [0.485, 0.456, 0.406]\n",
    "imagenet_std = [0.229, 0.224, 0.225]\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=imagenet_mean, std=imagenet_std)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look how it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# apply transform\n",
    "tensor_transformed = transform(im)\n",
    "# convert tensor to image\n",
    "im_transformed = tensor2im(tensor_transformed)\n",
    "# display results\n",
    "display_diff(im, im_transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"> **[PROBLEM II]**: </span> \n",
    "\n",
    "<span style=\"color:red\">Implement the function, which makes inverse transformation</span>\n",
    "\n",
    "<span style=\"color:blue\">look at realization of 'normalize' transform in</span>   https://github.com/pytorch/vision/blob/master/torchvision/transforms.py#L129  \n",
    "<span style=\"color:blue\"> consider to use in-place version of tensor operations as in the example of 'normalize' transform above </span>   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unnormalize(tensor, mean, std):    \n",
    "    \"\"\"\n",
    "    Make inverse transform to 'normalize'\n",
    "    \n",
    "    Args:\n",
    "        tensor (torch.Tensor): Tensor to unnormalize\n",
    "        mean (sequence)      : Sequence of means for R, G, B channels respectively.\n",
    "        std (sequence)       : Sequence of standard deviations for R, G, B channels respectively.  \n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: unnormalized tensor.\n",
    "    \"\"\"\n",
    "    #YOUR CODE HERE\n",
    "    for t, m, s in zip(tensor, mean, std):\n",
    "        t.mul_(s).add_(m)\n",
    "    return tensor "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\">when running the following cell you have to see identical images</span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "im_back = tensor2im(unnormalize(transform(im), imagenet_mean, imagenet_std))\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(im)\n",
    "plt.axis('off');\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(im_back)\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets and data loaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To work with datasets pytorch provides useful abstractions, like *dataset* and *dataloder* [https://github.com/pytorch/vision#datasets].  \n",
    "There are \n",
    "* prepared datasets, like MNIST, CIFAR10 and CIFAR100, COCO, etc.\n",
    "* *ImageFolder* dataset, which allows you to cook dataset for yourself without much efforts.\n",
    "\n",
    "\n",
    "The later is used here for DTD dataset and is especially useful when working with new data.\n",
    "\n",
    "On top of the *dataset* there is *dataloader*.\n",
    "The *dataloader* is used, as name suggests, to load the data.  \n",
    "It does that efficiently, with multi-threading, so you should not worry about how to feed you model with the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have a look at src/data_set.py where the *DataSetDTD* is defined.  \n",
    "There train & test dataloaders are bundled together, for convinence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from src.data_set import DataSetDTD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_set = DataSetDTD(path_data, num_dunkeys=4, batch_size=100, fin_scale=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To iterate over train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# When iteration starts, queue and thread start to load dataset from files.\n",
    "data_iter = iter(data_set.loader['train'])\n",
    "\n",
    "# Mini-batch images and labels.\n",
    "images, labels = data_iter.next()\n",
    "\n",
    "print (images.size())\n",
    "print (labels.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "To iterate over test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# When iteration starts, queue and thread start to load dataset from files.\n",
    "data_iter = iter(data_set.loader['test'])\n",
    "\n",
    "# Mini-batch images and labels.\n",
    "images, labels = data_iter.next()\n",
    "\n",
    "print (images.size())\n",
    "print (labels.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
